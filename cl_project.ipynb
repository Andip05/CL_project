{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andip05/CL_project/blob/main/cl_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ffa93404d58a1f32"
      },
      "id": "ffa93404d58a1f32"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.880381Z",
          "start_time": "2024-06-17T21:47:07.875157Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "from random import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Subset, TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ea1db9fecaaa9743"
      },
      "id": "ea1db9fecaaa9743"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.924558Z",
          "start_time": "2024-06-17T21:47:07.921811Z"
        },
        "id": "71d593a5df0d9530"
      },
      "id": "71d593a5df0d9530",
      "execution_count": 53
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "args = {\n",
        "    'bs': 128,\n",
        "    'epochs': 8,\n",
        "    'GAN_epochs': 10,\n",
        "    'num_tasks': 5,\n",
        "    'dataset': 'MNIST',\n",
        "    'num_classes': 10,\n",
        "    'in_size': 28,\n",
        "    'n_channels': 1,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 2e-4,\n",
        "    'lr_cl':1e-3,\n",
        "    'latent_dim':128\n",
        "}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.933013Z",
          "start_time": "2024-06-17T21:47:07.930218Z"
        },
        "id": "47961b79213f0979"
      },
      "id": "47961b79213f0979",
      "execution_count": 54
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module"
      ],
      "metadata": {
        "collapsed": false,
        "id": "81d07f4cd019ead"
      },
      "id": "81d07f4cd019ead"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and management"
      ],
      "metadata": {
        "collapsed": false,
        "id": "258e36825dfdb378"
      },
      "id": "258e36825dfdb378"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def get_dataset(dataroot, dataset):\n",
        "    if dataset == 'MNIST':\n",
        "        mean, std = (0.1307), (0.3081)\n",
        "    elif dataset == 'CIFAR10':\n",
        "        mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
        "\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "    train_dataset = torchvision.datasets.__dict__[dataset](\n",
        "        root=dataroot,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    val_dataset = torchvision.datasets.__dict__[dataset](\n",
        "        root=dataroot,\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def split_dataset(dataset, tasks_split):\n",
        "    split_dataset = {}\n",
        "    for e, current_classes in tasks_split.items():\n",
        "        task_indices = np.isin(np.array(dataset.targets), current_classes)\n",
        "        split_dataset[e] = Subset(dataset, np.where(task_indices)[0])\n",
        "    return split_dataset"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.941265Z",
          "start_time": "2024-06-17T21:47:07.937386Z"
        },
        "id": "f802c12131d47779"
      },
      "id": "f802c12131d47779",
      "execution_count": 55
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics & plotting"
      ],
      "metadata": {
        "collapsed": false,
        "id": "505625743f01f97e"
      },
      "id": "505625743f01f97e"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def dict2array(acc):\n",
        "    num_tasks = len(acc)\n",
        "    first_task = list(acc.keys())[0]\n",
        "    sequence_length = len(acc[first_task]) if isinstance(acc[first_task], list) else num_tasks\n",
        "    acc_array = np.zeros((num_tasks, sequence_length))\n",
        "    for task, val in acc.items():\n",
        "        acc_array[int(task), :] = val\n",
        "    return acc_array\n",
        "\n",
        "\n",
        "def plot_accuracy_matrix(array):\n",
        "    num_tasks = array.shape[1]\n",
        "    array = np.round(array, 2)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(array, vmin=np.min(array), vmax=np.max(array))\n",
        "    for i in range(len(array)):\n",
        "        for j in range(array.shape[1]):\n",
        "            ax.text(j,i, array[i,j], va='center', ha='center', c='w', fontsize=15)\n",
        "    ax.set_yticks(np.arange(num_tasks))\n",
        "    ax.set_ylabel('Number of tasks')\n",
        "    ax.set_xticks(np.arange(num_tasks))\n",
        "    ax.set_xlabel('Tasks finished')\n",
        "    ax.set_title(f\"ACC: {np.mean(array[:, -1]):.3f} -- std {np.std(np.mean(array[:, -1])):.3f}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_acc_over_time(array):\n",
        "    fig, ax = plt.subplots()\n",
        "    for e, acc in enumerate(array):\n",
        "        ax.plot(acc, label=e)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compute_average_accuracy(array):\n",
        "    num_tasks = len(array)\n",
        "    avg_acc = np.sum(array[:, -1], axis=0)/num_tasks\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "def compute_backward_transfer(array):\n",
        "    num_tasks = len(array)\n",
        "    diag = np.diag(array)[:-1] # Note, we do not compute backward transfer for the last task!\n",
        "    end_acc = array[:-1, -1]\n",
        "    bwt = np.sum(end_acc - diag)/(num_tasks - 1)\n",
        "    return bwt\n",
        "\n",
        "\n",
        "def compute_forward_transfer(array, b):\n",
        "    num_tasks = len(array)\n",
        "    sub_diag = np.diag(array, k=-1) # Note, we do not compute forward transfer for the first task!\n",
        "    fwt = np.sum(sub_diag - b[1:])/(num_tasks - 1)\n",
        "    return fwt"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.947816Z",
          "start_time": "2024-06-17T21:47:07.942229Z"
        },
        "id": "ff25002a00d339bc"
      },
      "id": "ff25002a00d339bc",
      "execution_count": 56
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "24ef5647b0305b73"
      },
      "id": "24ef5647b0305b73"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gen-Dis"
      ],
      "metadata": {
        "collapsed": false,
        "id": "dfd56561d30f9dcd"
      },
      "id": "dfd56561d30f9dcd"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(args['latent_dim'], 128),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(128, 256),\n",
        "            torch.nn.BatchNorm1d(256, 0.8),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(256, 512),\n",
        "            torch.nn.BatchNorm1d(512, 0.8),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(512, args['n_channels'] * args['in_size'] ** 2),\n",
        "            torch.nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        prv = self.model(z)\n",
        "        prv = prv.view(prv.size(0), args['n_channels'], args['in_size'], args['in_size'] )\n",
        "        return prv\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(args['n_channels'] * args['in_size'] ** 2, 512),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(512, 256),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.LeakyReLU(0.4),\n",
        "            torch.nn.Linear(128, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        prv = img.view(img.size(0), -1)\n",
        "        check = self.model(prv)\n",
        "        return check"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.962580Z",
          "start_time": "2024-06-17T21:47:07.958221Z"
        },
        "id": "7a497f9dfa922fd9"
      },
      "id": "7a497f9dfa922fd9",
      "execution_count": 57
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2489fde5ac71c7a1"
      },
      "id": "2489fde5ac71c7a1"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class GAN:\n",
        "    def __init__(self, loader):\n",
        "        self.loader = loader\n",
        "        self.generator = Generator().to(device)\n",
        "        self.discriminator = Discriminator().to(device)\n",
        "        self.adversarial_loss = nn.BCELoss()\n",
        "        self.opt_G = torch.optim.Adam(self.generator.parameters(), lr=args['lr'])\n",
        "        self.opt_D = torch.optim.Adam(self.discriminator.parameters(), lr=args['lr'])\n",
        "\n",
        "    def train_gan(self, loader):\n",
        "        pg = sum(p.numel() for p in self.generator.parameters() if p.requires_grad)\n",
        "        pd = sum(p.numel() for p in self.discriminator.parameters() if p.requires_grad)\n",
        "        print(\"Generator params: \", pg)\n",
        "        print(\"Disciminator params: \", pd)\n",
        "\n",
        "        for epoch in range(args['GAN_epochs']):\n",
        "            for i, (imgs, _) in enumerate(loader):\n",
        "                batch_size = imgs.size(0)\n",
        "\n",
        "                ones_true = torch.ones(batch_size, 1, requires_grad=False).to(device)\n",
        "                zeros_fake = torch.zeros(batch_size, 1, requires_grad=False).to(device)\n",
        "\n",
        "                real = imgs.to(device)\n",
        "\n",
        "                self.opt_G.zero_grad()\n",
        "                z = torch.randn(batch_size, args['latent_dim']).to(device)\n",
        "\n",
        "                gen = self.generator(z)\n",
        "                g_loss = self.adversarial_loss(self.discriminator(gen), ones_true)\n",
        "                g_loss.backward()\n",
        "                self.opt_G.step()\n",
        "\n",
        "                self.opt_D.zero_grad()\n",
        "                lossR = self.adversarial_loss(self.discriminator(real), ones_true)\n",
        "                lossF = self.adversarial_loss(self.discriminator(gen.detach()), zeros_fake)\n",
        "                d_loss = (lossR + lossF) / 2\n",
        "                d_loss.backward()\n",
        "                self.opt_D.step()\n",
        "\n",
        "    def generate_samples(self, n_samples):\n",
        "        self.generator.eval()\n",
        "        self.discriminator.eval()\n",
        "\n",
        "        generated_images = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(n_samples):\n",
        "                z = torch.randn(1, 100).to(device)\n",
        "                generated_image = self.generator(z).view(1, 28, 28).to(device)\n",
        "                generated_images.append(generated_image)\n",
        "\n",
        "        generated_images = torch.cat(generated_images)\n",
        "\n",
        "        return generated_images\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.971755Z",
          "start_time": "2024-06-17T21:47:07.966430Z"
        },
        "id": "9d3bdc1d69b0af9d"
      },
      "id": "9d3bdc1d69b0af9d",
      "execution_count": 58
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "collapsed": false,
        "id": "67ee66b7706f1ae2"
      },
      "id": "67ee66b7706f1ae2"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        hidden_size = args['hidden_size']\n",
        "        self.fc1 = torch.nn.Linear(args['in_size']**2 * args['n_channels'], hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(hidden_size, args['num_classes'])\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.flatten(start_dim=1)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        x = torch.nn.functional.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.975890Z",
          "start_time": "2024-06-17T21:47:07.972715Z"
        },
        "id": "95162d80804d8434"
      },
      "id": "95162d80804d8434",
      "execution_count": 59
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Incremental Agent"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1085f1bd461fcf6b"
      },
      "id": "1085f1bd461fcf6b"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, train_datasets, val_datasets):\n",
        "        self.model = MLP(args).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args['lr_cl'])\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.reset_acc()\n",
        "        self.train_datasets = train_datasets\n",
        "        self.val_datasets = val_datasets\n",
        "\n",
        "    def reset_acc(self):\n",
        "        self.acc = {key: [] for key in args['task_names']}\n",
        "        self.acc_end = {key: [] for key in args['task_names']}\n",
        "\n",
        "    def train(self):\n",
        "        datasets_new = list()\n",
        "        labels_new = list()\n",
        "        print(\"Classifier params: \", sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
        "\n",
        "        for task, data in self.train_datasets.items():\n",
        "            loader = DataLoader(data, batch_size=args['bs'], shuffle=True, drop_last=True)\n",
        "\n",
        "            gan = GAN(loader=loader)\n",
        "\n",
        "            gan.train_gan(loader)\n",
        "\n",
        "            for epoch in range(args['epochs']):\n",
        "                epoch_loss = 0\n",
        "                total = 0\n",
        "                correct = 0\n",
        "                for e, (X, y) in enumerate(loader):\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "\n",
        "                    if task == '0':\n",
        "                        X_new, y_new = X, y\n",
        "                    else:\n",
        "                        old_data = torch.cat(datasets_new, dim=0).to(device)\n",
        "                        old_labels = torch.cat(labels_new, dim=0).to(device)\n",
        "\n",
        "                        X_new = torch.cat([X, old_data], dim=0).to(device)\n",
        "                        y_new = torch.cat([y, old_labels], dim=0).to(device)\n",
        "\n",
        "\n",
        "                    output = self.model(X_new)\n",
        "                    loss = self.criterion(output, y_new)\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "                    correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y_new)\n",
        "                    total += len(X_new)\n",
        "\n",
        "                    if e % 50 == 0:\n",
        "                        self.validate()\n",
        "\n",
        "            self.validate(end_of_epoch=True)\n",
        "\n",
        "            z = torch.randn(len(loader.dataset), args['latent_dim']).to(device)\n",
        "            with torch.no_grad():\n",
        "                img = gan.generator(z).detach().to(device)\n",
        "                label = torch.argmax(self.model(img), dim=1).to(device)\n",
        "\n",
        "            datasets_new.append(img)\n",
        "            labels_new.append(label)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, end_of_epoch=False):\n",
        "        self.model.eval()\n",
        "        for task, data in self.val_datasets.items():\n",
        "            loader = torch.utils.data.DataLoader(data, batch_size=args['bs'], shuffle=True)\n",
        "            correct, total = 0, 0\n",
        "            for e, (X, y) in enumerate(loader):\n",
        "                if torch.cuda.is_available():\n",
        "                    X, y = X.cuda(), y.cuda()\n",
        "                output = self.model(X)\n",
        "                correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y).item()\n",
        "                total += len(X)\n",
        "            self.acc[task].append(correct/total)\n",
        "            if end_of_epoch:\n",
        "                self.acc_end[task].append(correct/total)\n",
        "        self.model.train()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.984428Z",
          "start_time": "2024-06-17T21:47:07.976627Z"
        },
        "id": "24541a30f677eaab"
      },
      "id": "24541a30f677eaab",
      "execution_count": 60
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "collapsed": false,
        "id": "24eab85ec391dbfa"
      },
      "id": "24eab85ec391dbfa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "collapsed": false,
        "id": "b7b350c81ecc730"
      },
      "id": "b7b350c81ecc730"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "classes = list(range(args['num_classes']))\n",
        "shuffle(classes)\n",
        "class_split = {str(i): classes[i*2: (i+1)*2] for i in range(args['num_tasks'])}\n",
        "args['task_names'] = list(class_split.keys())"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:07.987480Z",
          "start_time": "2024-06-17T21:47:07.985520Z"
        },
        "id": "cff22dfba4213e72"
      },
      "id": "cff22dfba4213e72",
      "execution_count": 61
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Go"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d1aa22b0954cd149"
      },
      "id": "d1aa22b0954cd149"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "collapsed": false,
        "id": "b0dec8d7ae9569e"
      },
      "id": "b0dec8d7ae9569e"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
        "train_tasks = split_dataset(train, class_split)\n",
        "val_tasks = split_dataset(test, class_split)\n",
        "agent = Agent(train_tasks, val_tasks)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:08.061763Z",
          "start_time": "2024-06-17T21:47:07.988095Z"
        },
        "id": "5a281481ff72ab4c"
      },
      "id": "5a281481ff72ab4c",
      "execution_count": 62
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "agent.validate()\n",
        "random_model_acc = [i[0] for i in agent.acc.values()]\n",
        "agent.reset_acc()\n",
        "agent.train()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:22.893990Z",
          "start_time": "2024-06-17T21:47:08.064029Z"
        },
        "id": "f772442021559de3"
      },
      "id": "f772442021559de3",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "acc_at_end_arr = dict2array(agent.acc_end)\n",
        "plot_accuracy_matrix(acc_at_end_arr)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-17T21:47:22.895251Z",
          "start_time": "2024-06-17T21:47:22.895176Z"
        },
        "id": "c3aafd1198b006cb"
      },
      "id": "c3aafd1198b006cb",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "acc_arr = dict2array(agent.acc)\n",
        "plot_acc_over_time(acc_arr)"
      ],
      "metadata": {
        "id": "95d7215c880d5680"
      },
      "id": "95d7215c880d5680",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "print(f\"The average accuracy at the end of sequence is: {compute_average_accuracy(acc_at_end_arr):.3f}\")\n",
        "print(f\"BWT:'{compute_backward_transfer(acc_at_end_arr):.3f}'\")\n",
        "print(f\"FWT:'{compute_forward_transfer(acc_at_end_arr, random_model_acc):.3f}'\")"
      ],
      "metadata": {
        "id": "2b48c81153682280"
      },
      "id": "2b48c81153682280",
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}